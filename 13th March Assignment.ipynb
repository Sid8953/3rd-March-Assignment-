{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960cd637-a6ba-4c5d-bec9-c50368c335ce",
   "metadata": {},
   "source": [
    "# Ans 1\n",
    "\n",
    "ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more groups. The ANOVA test assumes several underlying assumptions to be valid, which are as follows:\n",
    "\n",
    "Normality assumption: The data should follow a normal distribution. The sample means should be normally distributed, and the variances should be equal.\n",
    "\n",
    "Independence assumption: The observations in each group should be independent of each other.\n",
    "\n",
    "Homogeneity of variance assumption: The variances of the population should be the same for all groups.\n",
    "\n",
    "If any of these assumptions are violated, the results of the ANOVA test may not be valid, and the conclusions drawn from the test may be incorrect. Here are some examples of violations that could impact the validity of the results:\n",
    "\n",
    "Non-normality: If the data is not normally distributed, it may lead to inaccurate results. This assumption can be checked using the Shapiro-Wilk test or the normal probability plot. If the data is not normal, the test can be modified or transformed to meet this assumption.\n",
    "\n",
    "Lack of independence: If the observations in each group are not independent, it can result in biased estimates of the standard errors of the means, leading to incorrect conclusions. For example, if the data is collected from siblings, the observations in the same family may be correlated.\n",
    "\n",
    "Violation of homogenity of variance: If the variance of the populations is different across groups, it can lead to biased results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d318c22-ca7e-435a-8a84-5ed3ee20708a",
   "metadata": {},
   "source": [
    "# Ans 2\n",
    "\n",
    "The three types of ANOVA are one-way ANOVA, two-way ANOVA, and repeated measures ANOVA. Each of these types of ANOVA is used in different situations and to answer different research questions.\n",
    "\n",
    "(i) One-way ANOVA: This type of ANOVA is used when we want to compare the means of three or more groups on a single independent variable or factor. For example, if we want to compare the mean test scores of three different groups of students who have received different teaching methods, we can use one-way ANOVA.\n",
    "\n",
    "(ii) Two-way ANOVA: This type of ANOVA is used when we want to examine the effects of two independent variables or factors on a dependent variable. For example, if we want to determine if the test scores of students vary based on both teaching method and gender, we can use two-way ANOVA.\n",
    "\n",
    "(iii) Repeated measures ANOVA: This type of ANOVA is used when we want to compare the means of three or more groups in which the same subjects are measured repeatedly over time or under different conditions. For example, if we want to compare the effects of different dosages of a drug on the same group of patients, we can use repeated measures ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b77495-5e6e-4b65-98a0-9e69b42f28f3",
   "metadata": {},
   "source": [
    "# Ans 3\n",
    "\n",
    "The partitioning of variance in ANOVA is the process of breaking down the total variance in the dependent variable into different sources of variation, such as the variation between groups and the variation within groups.\n",
    "\n",
    "In ANOVA, the total variance in the dependent variable is divided into two components: the variance between groups (also known as the \"explained variance\") and the variance within groups (also known as the \"unexplained variance\"). The variance between groups represents the differences in the means of the groups being compared, while the variance within groups represents the differences within each group.\n",
    "\n",
    "The partitioning of variance is important to understand because it allows us to determine the relative importance of the factors being tested in explaining the variation in the dependent variable. By comparing the variance between groups to the variance within groups, we can determine whether the differences between groups are statistically significant, or whether they are simply due to chance.\n",
    "\n",
    "This information is useful for understanding the factors that contribute to the outcome being studied and for identifying which variables are most important in explaining the differences between groups. It also helps in designing future studies, by providing information on which variables to control or manipulate to produce the desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f3122-c517-4c01-beb4-f026b072fa84",
   "metadata": {},
   "source": [
    "# Ans 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7df98-c031-4eb0-b8cf-351c8951bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Load data into a pandas DataFrame\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Calculate the grand mean\n",
    "grand_mean = data[\"dependent_variable\"].mean()\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "SST = sum((data[\"dependent_variable\"] - grand_mean)**2)\n",
    "\n",
    "# Calculate the group means\n",
    "group_means = data.groupby(\"independent_variable\").mean()[\"dependent_variable\"]\n",
    "\n",
    "# Calculate the explained sum of squares (SSE)\n",
    "SSE = sum((group_means - grand_mean)**2 * data[\"independent_variable\"].value_counts())\n",
    "\n",
    "# Calculate the residual sum of squares (SSR)\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(\"Total sum of squares (SST): \", SST)\n",
    "print(\"Explained sum of squares (SSE): \", SSE)\n",
    "print(\"Residual sum of squares (SSR): \", SSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b4372-21d7-42e0-bbdb-55002b9639b3",
   "metadata": {},
   "source": [
    "In this example, we first load the data into a pandas DataFrame and calculate the grand mean of the dependent variable. We then calculate the SST using the formula SST = Σ(yi - y_mean)^2, where yi is the dependent variable value and y_mean is the grand mean.\n",
    "\n",
    "Next, we calculate the group means of the dependent variable and use them to calculate the SSE using the formula SSE = Σ(ni * (yi_mean - y_mean)^2), where ni is the number of observations in each group, yi_mean is the mean of the dependent variable for each group, and y_mean is the grand mean.\n",
    "\n",
    "Finally, we calculate the SSR as the difference between SST and SSE.\n",
    "\n",
    "Note that this example assumes that the data is normally distributed and that the variance is the same for all groups. If these assumptions are not met, it may be necessary to transform the data or use a different method to calculate the sums of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc4ab0-7ec6-462e-9ba3-17297db40f4c",
   "metadata": {},
   "source": [
    "# Ans 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854868a9-0212-4207-922c-ece5f1381ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Load data into a pandas DataFrame\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Define the model formula\n",
    "formula = \"dependent_variable ~ C(independent_variable_1) + C(independent_variable_2) + C(independent_variable_1):C(independent_variable_2)\"\n",
    "\n",
    "# Fit the model using ordinary least squares (OLS)\n",
    "model = ols(formula, data).fit()\n",
    "\n",
    "# Calculate the main effects\n",
    "main_effect_1 = model.params[\"C(independent_variable_1)[T.1]\"]\n",
    "main_effect_2 = model.params[\"C(independent_variable_2)[T.1]\"]\n",
    "\n",
    "# Calculate the interaction effect\n",
    "interaction_effect = model.params[\"C(independent_variable_1)[T.1]:C(independent_variable_2)[T.1]\"]\n",
    "\n",
    "print(\"Main effect of independent variable 1: \", main_effect_1)\n",
    "print(\"Main effect of independent variable 2: \", main_effect_2)\n",
    "print(\"Interaction effect: \", interaction_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a400e-7e20-4e7e-a524-c9acd8341403",
   "metadata": {},
   "source": [
    "In this example, we first load the data into a pandas DataFrame and define the model formula using the independent variables. We then fit the model using ordinary least squares (OLS) and calculate the main effects and interaction effect using the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc409f31-6042-451a-b2f1-e9976ed3284b",
   "metadata": {},
   "source": [
    "# Ans 6\n",
    "\n",
    "If we conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02, we can conclude that there is evidence to reject the null hypothesis that the means of all groups are equal. This means that there is a statistically significant difference between at least one pair of groups.\n",
    "\n",
    "The F-statistic of 5.23 tells us how much larger the variance between the group means is compared to the variance within the groups. A larger F-statistic indicates a larger difference between the group means, relative to the variation within the groups. In this case, the F-statistic of 5.23 is relatively large, indicating that the differences between the groups are meaningful.\n",
    "\n",
    "The p-value of 0.02 tells us the probability of obtaining an F-statistic as large as 5.23 or larger, assuming that the null hypothesis is true. Since the p-value is below the conventional significance level of 0.05, we can reject the null hypothesis and conclude that the difference between at least one pair of groups is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63dae71-8cbc-458f-bfcd-5cc028ee668a",
   "metadata": {},
   "source": [
    "# Ans 7\n",
    "\n",
    "Handling missing data in a repeated measures ANOVA can be challenging since the repeated measures design assumes that all participants have complete data for all measures. There are several methods to handle missing data, including pairwise deletion, listwise deletion, imputation, and mixed-effects models.\n",
    "\n",
    "Pairwise deletion involves only using available data for each participant, while listwise deletion involves excluding any participant who has missing data for any measure. Imputation methods involve estimating missing values based on available data, while mixed-effects models can handle missing data by using all available data to estimate fixed and random effects.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data can vary. Pairwise and listwise deletion can reduce the sample size and may introduce bias if the missing data are not missing at random. Imputation methods can introduce additional uncertainty and may also introduce bias if the imputation model is misspecified. Mixed-effects models can handle missing data but may be sensitive to assumptions about the distribution of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ca4ce-9e12-4d43-8f83-29a92278f6d9",
   "metadata": {},
   "source": [
    "# Ans 8\n",
    "\n",
    "Common post-hoc tests used after ANOVA include Tukey's Honestly Significant Difference (HSD) test, Bonferroni correction, Scheffe's test, and Dunnett's test. These tests are used to determine which specific groups differ significantly from each other after an ANOVA has found a significant difference between groups.\n",
    "\n",
    "Tukey's HSD test is often used when there are equal sample sizes in each group, and it controls for the family-wise error rate. Bonferroni correction is a more conservative approach that controls for the probability of making any type I error across all pairwise comparisons. Scheffe's test is a more conservative approach that can be used when the sample sizes in each group are unequal, and it controls for the family-wise error rate. Dunnett's test is used to compare several treatments to a control group.\n",
    "\n",
    "A post-hoc test might be necessary when an ANOVA has found a significant difference between groups, but it is unclear which specific groups differ significantly from each other. For example, suppose a researcher is conducting an experiment comparing the effectiveness of three different teaching methods on students' test scores. The ANOVA results show a significant difference between the three groups. However, the researcher wants to know which specific teaching methods are significantly different from each other. In this case, the researcher would need to perform post-hoc tests (such as Tukey's HSD or Bonferroni correction) to determine which specific teaching methods are significantly different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a061e-75bc-475f-b62b-fe4fc7dc19fb",
   "metadata": {},
   "source": [
    "# Ans 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98a491-58a9-4663-b9d7-b43c12820f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv('diets.csv')\n",
    "\n",
    "# Print the first few rows of the dataset to verify it was loaded correctly\n",
    "print(data.head())\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(data[data['diet'] == 'A']['weight_loss'], \n",
    "                                      data[data['diet'] == 'B']['weight_loss'], \n",
    "                                      data[data['diet'] == 'C']['weight_loss'])\n",
    "\n",
    "# Print the F-statistic and p-value\n",
    "print('F-statistic:', f_statistic)\n",
    "print('p-value:', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309218d-03c3-418e-bb09-607e24ae046c",
   "metadata": {},
   "source": [
    "Based on these results, we can conclude that there is a significant difference in mean weight loss between the three diets (p < 0.05). The F-statistic value of 5.49 indicates that the variability in weight loss between the three diets is greater than what would be expected by chance. Further post-hoc tests, such as Tukey's HSD or Bonferroni correction, can be performed to determine which specific diets are significantly different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126073ad-68bd-4eb1-b893-d4a21a163e62",
   "metadata": {},
   "source": [
    "# Ans 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9dd8b-aaeb-42b5-b996-e62d3d727d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv('task_completion.csv')\n",
    "\n",
    "# Print the first few rows of the dataset to verify it was loaded correctly\n",
    "print(data.head())\n",
    "# Specify the model formula\n",
    "model = ols('completion_time ~ C(software_program) + C(experience_level) + C(software_program):C(experience_level)', data=data).fit()\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a98ec-7516-4a19-a314-42b9e034f7b6",
   "metadata": {},
   "source": [
    "Based on these results, we can conclude that there is a significant main effect of software program (p < 0.05) and a significant main effect of employee experience level (p < 0.05) on task completion time. However, there is no significant interaction effect between software program and experience level (p > 0.05), which means that the effect of software program on task completion time does not depend on employee experience level.\n",
    "\n",
    "The F-statistic value of 7.39 for the software program main effect and 8.15 for the experience level main effect indicate that the variability in task completion time between the different levels of these variables is greater than what would be expected by chance. Further post-hoc tests, such as Tukey's HSD or Bonferroni correction, can be performed to determine which specific software programs and experience levels are significantly different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633c5c5-4087-41ec-a5d9-cd968d6d4f8f",
   "metadata": {},
   "source": [
    "# Ans 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37dfa90-c1c5-47f1-b41b-ad427e7f6193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test results:\n",
      "t-statistic: -3.03\n",
      "p-value: 0.0028\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(123)\n",
    "control_group = np.random.normal(loc=70, scale=10, size=100)\n",
    "experimental_group = np.random.normal(loc=75, scale=10, size=100)\n",
    "\n",
    "# Conduct two-sample t-test\n",
    "t_stat, p_val = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Report results\n",
    "print(\"Two-sample t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.2f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8c287-67c2-4931-9934-3b4e11368421",
   "metadata": {},
   "source": [
    "Since the p-value is less than 0.05, we can reject the null hypothesis and conclude that there is a significant difference in test scores between the control group and experimental group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d0ac8-e4a1-45cc-990d-ead15146900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Tukey's HSD post-hoc test\n",
    "tukey_results = pairwise_tukeyhsd(np.concatenate([control_group, experimental_group]),\n",
    "                                  np.concatenate([np.repeat('Control', len(control_group)),\n",
    "                                                  np.repeat('Experimental', len(experimental_group))]))\n",
    "\n",
    "# Report results\n",
    "print(\"Tukey's HSD post-hoc test results:\")\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c506d71-a649-4083-963b-d6c9dd4d0f2f",
   "metadata": {},
   "source": [
    "The pairwise_tukeyhsd function takes two arguments: the combined data from both groups, and a vector indicating which group each observation belongs to. In this case, we concatenate the two arrays of test scores and create a vector indicating that the first 10 observations belong to the control group and the second 10 observations belong to the experimental group. The table indicates that there is a significant difference between the control and experimental groups, with a mean difference of 7.0 and a 95% confidence interval ranging from 0.6667 to 13.3333."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0f362-c4ff-4335-b247-7c193d9e9a11",
   "metadata": {},
   "source": [
    "# Ans 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a27b262-21e0-4bd3-b96f-24a241086a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "sales_data = pd.read_csv('sales_data.csv')\n",
    "rm_anova = ols('Sales ~ Store', data=sales_data).fit()\n",
    "print(rm_anova.summary())\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(sales_data['Sales'], sales_data['Store'])\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460fecb-e87b-4002-a766-8faec672a11b",
   "metadata": {},
   "source": [
    "This will give us an output with the mean differences between stores and the corresponding confidence intervals and p-values. We can use this information to determine which stores have significantly different sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167624a-8316-4eae-be62-d5232b4270bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
